{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Conservation Gym Challenge\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/cboettig/rl-minicourse/blob/main/challenge.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "\n",
    "In this example, we set up a generic three species, two action problem, and illustrate how to provide a custom population dynamics function, action function, and utility function to represent a caribou conservation objective.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we'll need these packages to begin\n",
    "# !pip install stable-baselines3 plotnine polars sb3_contrib tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Caribou Conservation\n",
    "\n",
    "We consider a scenario where wolves, $Z$, prey on both caribou, $Y$ and moose, $X$, while caribou and moose compete for resources:\n",
    "\n",
    "$$X_{t+1} = X_t + r_x X \\left( 1 -  \\frac{X +\\tau_{xy} Y}{K} \\right) - \\frac{(1-D) \\beta Z X^2}{v_0^2 + X^2} + \\sigma_x X \\xi_x$$\n",
    "\n",
    "$$Y_{t+1} = Y_t + r_y Y \\left( 1 - \\frac{Y +\\tau_{yx} X}{K} \\right) - \\frac{D \\beta Z X^2}{v_0^2 + Y^2} + \\sigma_y Y \\xi_y$$\n",
    "\n",
    "$$Z_{t+1} = Z_t + \\alpha \\beta Z \\left( \\frac{(1-D) X^2}{v_0^2 + X^2}  +  \\frac{D Y^2}{v_0^2 + Y^2}  \\right) - d_H Z + \\sigma_z Z \\xi_z$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# pop = moose, caribou, wolves\n",
    "# Caribou Scenario\n",
    "def dynamics(pop, effort, harvest_fn, p, timestep=1):\n",
    "\n",
    "    pop = harvest_fn(pop, effort)        \n",
    "    X, Y, Z = pop[0], pop[1], pop[2]\n",
    "    \n",
    "    K = p[\"K\"] # - 0.2 * np.sin(2 * np.pi * timestep / 3200)\n",
    "    D = p[\"D\"] + 0.5 * np.sin(2 * np.pi * timestep / 3200)\n",
    "    beta = p[\"beta\"] + 0.2 * np.sin(2 * np.pi * timestep / 3200)\n",
    "\n",
    "    X += (p[\"r_x\"] * X * (1 - (X + p[\"tau_xy\"] * Y) / K)\n",
    "            - (1 - D) * beta * Z * (X**2) / (p[\"v0\"]**2 + X**2)\n",
    "            + p[\"sigma_x\"] * X * np.random.normal()\n",
    "            )\n",
    "    \n",
    "    Y += (p[\"r_y\"] * Y * (1 - (Y + p[\"tau_yx\"]* X ) / K )\n",
    "            - D * beta * Z * (Y**2) / (p[\"v0\"]**2 + Y**2)\n",
    "            + p[\"sigma_y\"] * Y * np.random.normal()\n",
    "            )\n",
    "\n",
    "    Z += p[\"alpha\"] * beta * Z * (\n",
    "            (1-D) * (X**2) / (p[\"v0\"]**2 + X**2)\n",
    "            + D * (Y**2) / (p[\"v0\"]**2 + Y**2)\n",
    "            ) - p[\"dH\"] * Z +  p[\"sigma_z\"] * Z  * np.random.normal()\n",
    "    \n",
    "    pop = np.array([X, Y, Z], dtype=np.float32)\n",
    "    pop = np.clip(pop, [0,0,0], [np.Inf, np.Inf, np.Inf])\n",
    "    return(pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "initial_pop = [0.5, 0.5, 0.2]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "\"r_x\": np.float32(0.13),\n",
    "\"r_y\": np.float32(0.2),\n",
    "\"K\": np.float32(1),\n",
    "\"beta\": np.float32(.1),\n",
    "\"v0\":  np.float32(0.1),\n",
    "\"D\": np.float32(0.8),\n",
    "\"tau_yx\": np.float32(0.7),\n",
    "\"tau_xy\": np.float32(0.2),\n",
    "\"alpha\": np.float32(.4), \n",
    "\"dH\": np.float32(0.03),\n",
    "\"sigma_x\": np.float32(0.05),\n",
    "\"sigma_y\": np.float32(0.05),\n",
    "\"sigma_z\": np.float32(0.05)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also define the dynamics of the action, a 'harvest' or culling function.  In this scenario, we imagine that we can cull either the moose or wolf population (or both).  We assume our control action introduces a percent mortality equal to the control effort applied times a catachability coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest(pop, effort):\n",
    "    q0 = 0.5 # catchability coefficients -- erradication is impossible\n",
    "    q2 = 0.5\n",
    "    pop[0] = pop[0] * (1 - effort[0] * q0) # pop 0, moose\n",
    "    pop[2] = pop[2] * (1 - effort[1] * q2) # pop 2, wolves\n",
    "    return pop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to define the utility or reward derived from taking these actions under this population state.  In this scenario, our population control actions are costly, while we acrue a benefit proportional to the size of the current caribou population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility(pop, effort):\n",
    "    benefits = 0.5 * pop[1] # benefit from Caribou\n",
    "    costs = .00001 * (effort[0] + effort[1]) # cost to culling\n",
    "    if np.any(pop <= 0.01):\n",
    "        benefits -= 1\n",
    "    return benefits - costs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate our environment and allow RL algorithms to train on this environment, we define a simple python class using the gym module.  This class defines the possible action space as two continuously-valued action variables (culling effort of moose and wolves respectively), and three continuously valued state variables (population of moose, caribou and wolves).  To improve performance of RL training, it is necessary to transform the continuous space to -1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's3a2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# verify that the environment is defined correctly    \u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv_checker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_env\n\u001b[0;32m---> 78\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43ms3a2\u001b[49m()\n\u001b[1;32m     79\u001b[0m check_env(env, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \n",
      "\u001b[0;31mNameError\u001b[0m: name 's3a2' is not defined"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "class caribou(gym.Env):\n",
    "    \"\"\"A 3-species ecosystem model with two control actions\"\"\"\n",
    "    def __init__(self, config=None):\n",
    "        config = config or {}\n",
    "                                \n",
    "        ## these parameters may be specified in config                                  \n",
    "        self.Tmax = config.get(\"Tmax\", 800)\n",
    "        self.threshold = config.get(\"threshold\", np.float32(1e-4))\n",
    "        self.init_sigma = config.get(\"init_sigma\", np.float32(1e-3))\n",
    "        self.training = config.get(\"training\", True)\n",
    "        self.initial_pop = config.get(\"initial_pop\", initial_pop)\n",
    "        self.parameters = config.get(\"parameters\", parameters)\n",
    "        self.dynamics = config.get(\"dynamics\", dynamics)\n",
    "        self.harvest = config.get(\"harvest\", harvest)\n",
    "        self.utility = config.get(\"utility\", utility)\n",
    "        self.observe = config.get(\"observe\", lambda state: state) # default to perfectly observed case\n",
    "        self.bound = 2 * self.parameters[\"K\"]\n",
    "        \n",
    "        self.action_space = gym.spaces.Box(\n",
    "            np.array([-1, -1], dtype=np.float32),\n",
    "            np.array([1, 1], dtype=np.float32),\n",
    "            dtype = np.float32\n",
    "        )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            np.array([-1, -1, -1], dtype=np.float32),\n",
    "            np.array([1, 1, 1], dtype=np.float32),\n",
    "            dtype=np.float32,\n",
    "        )        \n",
    "        self.reset(seed=config.get(\"seed\", None))\n",
    "\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        self.timestep = 0\n",
    "        self.initial_pop += np.multiply(self.initial_pop, np.float32(self.init_sigma * np.random.normal(size=3)))\n",
    "        self.state = self.state_units(self.initial_pop)\n",
    "        info = {}\n",
    "        return self.observe(self.state), info\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
    "        pop = self.population_units(self.state) # current state in natural units\n",
    "        effort = (action + 1.) / 2\n",
    "\n",
    "        # harvest and recruitment\n",
    "        reward = self.utility(pop, effort)\n",
    "        nextpop = self.dynamics(pop, effort, self.harvest, self.parameters, self.timestep)\n",
    "        \n",
    "        self.timestep += 1\n",
    "        terminated = bool(self.timestep > self.Tmax)\n",
    "        \n",
    "        # in training mode only: punish for population collapse\n",
    "        if any(pop <= self.threshold) and self.training:\n",
    "            terminated = True\n",
    "            reward -= 50/self.timestep\n",
    "        \n",
    "        self.state = self.state_units(nextpop) # transform into [-1, 1] space\n",
    "        observation = self.observe(self.state) # same as self.state\n",
    "        return observation, reward, terminated, False, {}\n",
    "    \n",
    "    def state_units(self, pop):\n",
    "        self.state = 2 * pop / self.bound - 1\n",
    "        self.state = np.clip(self.state,  \n",
    "                             np.repeat(-1, self.state.__len__()), \n",
    "                             np.repeat(1, self.state.__len__()))\n",
    "        return np.float32(self.state)\n",
    "    \n",
    "    def population_units(self, state):\n",
    "        pop = (state + 1) * self.bound /2\n",
    "        return np.clip(pop, \n",
    "                       np.repeat(0, pop.__len__()),\n",
    "                       np.repeat(np.Inf, pop.__len__()))\n",
    "    \n",
    "# verify that the environment is defined correctly    \n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = s3a2()\n",
    "check_env(env, warn=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.array([-1,-1])\n",
    "action = np.array([-.7,-.97])\n",
    "\n",
    "df = []\n",
    "episode_reward = 0\n",
    "observation, _ = env.reset()\n",
    "for t in range(env.Tmax):\n",
    "  obs = env.population_units(observation) # natural units\n",
    "  df.append([t, episode_reward, *obs])\n",
    "  observation, reward, terminated, done, info = env.step(action)\n",
    "  episode_reward += reward\n",
    "#  if terminated or done:\n",
    "#    break\n",
    "\n",
    "episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import polars as pl\n",
    "from plotnine import ggplot, aes, geom_line\n",
    "cols = [\"t\", \"reward\", \"X\", \"Y\", \"Z\"]\n",
    "\n",
    "dfl = (pl.DataFrame(df, schema=cols).\n",
    "        select([\"t\", \"X\", \"Y\", \"Z\"]).\n",
    "        melt(\"t\")\n",
    "      )\n",
    "ggplot(dfl, aes(\"t\", \"value\", color=\"variable\")) + geom_line()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC, ARS\n",
    "from stable_baselines3 import PPO, A2C, DQN, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_env = make_vec_env(caribou, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARS(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/jovyan/logs\")\n",
    "model.learn(total_timesteps=2_000_000)\n",
    "model.save(\"ars_caribou\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/jovyan/logs\", use_sde=True, device = \"cpu\")\n",
    "model.learn(total_timesteps=2_000_000)\n",
    "model.save(\"ppo_caribou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TQC(\"MlpPolicy\", env, verbose=0, tensorboard_log=\"/home/jovyan/logs\", use_sde=True)\n",
    "model.learn(total_timesteps=1_000_000)\n",
    "model.save(\"tqc_caribou\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env)\n",
    "agent = model.load(\"ppo_caribou\")\n",
    "\n",
    "def sim(agent, env):\n",
    "    \n",
    "    df = []\n",
    "    episode_reward = 0\n",
    "    observation, _ = env.reset()\n",
    "    \n",
    "    for t in range(env.Tmax):\n",
    "      action, _ = agent.predict(observation, deterministic=True)\n",
    "      obs = env.population_units(observation) # natural units\n",
    "      effort = (action + 1)/2\n",
    "      df.append([t, episode_reward, *effort, *obs])\n",
    "      \n",
    "      observation, reward, terminated, done, info = env.step(action)\n",
    "      episode_reward += reward\n",
    "      if terminated or done:\n",
    "        break\n",
    "    \n",
    "    return df, episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optional plotting code\n",
    "import polars as pl\n",
    "from plotnine import ggplot, aes, geom_line\n",
    "\n",
    "def plot_rl_sim(df):\n",
    "    cols = [\"t\", \"reward\", \"moose_cull\", \"wolf_cull\", \"X\", \"Y\", \"Z\"]\n",
    "    \n",
    "    dfl = (pl.DataFrame(df, schema=cols).\n",
    "            select([\"t\", \"moose_cull\", \"wolf_cull\", \"X\", \"Y\", \"Z\"]).\n",
    "            unpivot(index = \"t\")\n",
    "          )\n",
    "    return ggplot(dfl, aes(\"t\", \"value\", color=\"variable\")) + geom_line()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env, device = \"cpu\")\n",
    "agent = model.load(\"ppo_caribou\", device = \"cpu\")\n",
    "df, reward = sim(agent, env)\n",
    "print(reward)\n",
    "plot_rl_sim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TQC(\"MlpPolicy\", env)\n",
    "agent = model.load(\"tqc_caribou\")\n",
    "\n",
    "df, reward = sim(agent, env)\n",
    "print(reward)\n",
    "plot_rl_sim(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
