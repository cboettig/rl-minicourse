{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Management strategy evaluation\n",
    "\n",
    "Rather than experimenting with a handful of strategies, we can seek to optimize a strategy.  Here we introduce a technique for multi-dimensional optimization of nonlinear, stochastic systems based on Gaussian processes.  While we have so far considered agents whose behavior is set by a single parameter, this approach is also suitable for more complex policies involving multiple parameters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fish import fish\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skopt import gp_minimize, gbrt_minimize\n",
    "from skopt.plots import plot_objective, plot_convergence, plot_gaussian_process\n",
    "from utils import simulate\n",
    "from utils import plot_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = fish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple agent\n",
    "class some_agent:\n",
    "    def __init__(self, effort):\n",
    "        self.effort = effort\n",
    "\n",
    "    def predict(self, obs, **kwargs):\n",
    "        return self.effort\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve convergence, we will define a optimization function that computes the average reward over 100 simulations.  Becuase the optimizer seeks to 'minimize' the value, we will also need to take the negative of the average episode reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def g(x):\n",
    "    agent = some_agent(x)\n",
    "    def my_function(i):\n",
    "        np.random.seed(i)\n",
    "        df, mu = simulate(agent, env, timeseries=False)\n",
    "        return mu\n",
    "    # do 100 simulations at each value to reduce noise    \n",
    "    results = [my_function(i) for i in range(100)]\n",
    "    return -np.mean(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# look for actions between [0,1] \n",
    "res = gp_minimize(g, [(0, .2)], n_calls = 20, verbose=True, n_jobs=-1)\n",
    "res.fun, res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = some_agent(*res.x)\n",
    "df, mu = simulate(agent, env)\n",
    "print(mu)\n",
    "plot_sim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax2 = plot_convergence(res)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ax2 = plot_gaussian_process(res)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach still requires that we define the agent's general behavior.  In principle, an agent could take a different response based on any possible observation -- that is, any map, `action_t = agent(observation_t)`.  Neural networks have consistently proven to be highly flexible function approximators given ample data.  In our next section we will seek to use neural networks as general purpose maps from observation space to action space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import TQC, ARS\n",
    "from stable_baselines3 import PPO, A2C, DQN, SAC, TD3\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_env = make_vec_env(fish, 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARS(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/jovyan/logs\")\n",
    "model.learn(total_timesteps=800_000, tb_log_name=\"ars-fish\", progress_bar=True)\n",
    "model.save(\"ars_fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import policy_fn\n",
    "from utils import simulate_rl\n",
    "\n",
    "model = ARS(\"MlpPolicy\", env, device=\"cpu\")\n",
    "agent = model.load(\"ars_fish\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df, mu = simulate_rl(agent, env)\n",
    "print(mu)\n",
    "plot_sim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/jovyan/logs\", use_sde=True, device = \"cpu\")\n",
    "model.learn(total_timesteps=800_000, tb_log_name=\"ppo-fish\", progress_bar=True)\n",
    "model.save(\"ppo_fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results\n",
    "model = PPO(\"MlpPolicy\", env, device=\"cpu\")\n",
    "agent = model.load(\"ppo_fish\")\n",
    "\n",
    "df, mu = simulate_rl(agent, env)\n",
    "print(mu)\n",
    "plot_sim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_fn(agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TQC(\"MlpPolicy\", vec_env, verbose=0, tensorboard_log=\"/home/jovyan/logs\", use_sde=True, device = \"cuda\")\n",
    "model.learn(total_timesteps=200_000, tb_log_name=\"tqc-fish\", progress_bar=False)\n",
    "model.save(\"tqc_fish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TQC(\"MlpPolicy\", env, device = \"cpu\")\n",
    "agent = model.load(\"tqc_fish\")\n",
    "\n",
    "df, mu = simulate_rl(agent, env)\n",
    "print(mu)\n",
    "plot_sim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_fn(agent, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
