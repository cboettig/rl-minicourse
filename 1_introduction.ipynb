{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94f836f-57d0-4d34-aa6b-4dae4b869ddc",
   "metadata": {},
   "source": [
    "# Sequential decision problems in ecology: a hands-on introduction\n",
    "\n",
    "Where do models come from? Why do we create them, and what purpose do they serve? Ecological processes are complex, models seek only to capture salient features at spatial, temporal and organization scales considered relevant.\n",
    "\n",
    "Process-based simulations provide an excellent test case of our methods for modeling and decision-making. In real world empirical systems, we never have access to a 'true' model -- multiple models with substantially different implications may 'fit' available data equally well, and a model that has appeared to match empricial data to date can suddenly be wrong.  There is no such thing as 'verifying' or 'validating' a model -- all models are approximations -- leaky abstractions of more complex processes.  But applied to a simulation, a generative process, we can come close.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14aa07-9418-4c25-a941-44e011b5e4cc",
   "metadata": {},
   "source": [
    "Here we consider a simulation of a fish population from which we wish to extract a sustainable harvest.  The underlying model is not visible to us, instead we can empirically observe and manipulate the system to try and come up with a model of the system behavior.  We could use such a model to predict future values of the system, speculate about mechanisms driving the dynamics, and determine an optimal harvesting strategy.  Here is our model 'environment':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4dead2-af90-4518-a92b-b475df62ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from envs.fish import fish\n",
    "env = fish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50df7e4-8e41-4081-b28d-8882f4f3f586",
   "metadata": {},
   "source": [
    "We can simulate a step forward in time of this environment using the time step method.  This takes an optional 'harvest effort' and returns an observation of the population in the next time step, as well as a 'utility' or reward from any harvest, and an indicator of whether we have hit a \"game over\" condition such as extinction of the population:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc873728-926b-4f9b-ac1d-3823c0734473",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.time_step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac611907-19a9-4ac2-a4fe-07b859a9846a",
   "metadata": {},
   "source": [
    "We can rerun the step above again and again to go further forward in time. Each run updates the internal state of the system, returning the most recent observation.  \n",
    "\n",
    "Let's try a simulation of a few timesteps of underlying process under a high fishing rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf3d06-a471-4f5f-9037-1d0cb8ce121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for t in range(0,10):\n",
    "    obs, reward, terminated = env.time_step(.1)\n",
    "    print(obs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94088cf-4356-4f47-aa51-3c3b52031c0c",
   "metadata": {},
   "source": [
    "Let's plot these values to get a better sense of the overall dynamics.  The code below considers no harvest effort, `action = 0`, and naturally the reward derived from this is zero.  After exploring this case, consider various different harvest policies to see which produces the greatest reward.  Is the most aggressive harvest strategy the most profitable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e84344e-181c-4fb7-b9de-2df0186e6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_sim\n",
    "\n",
    "action = 0.\n",
    "\n",
    "# initial conditions\n",
    "df = []\n",
    "episode_reward = 0\n",
    "env.reset()\n",
    "\n",
    "# simulate for Tmax timesteps:\n",
    "for t in range(env.Tmax):\n",
    "  obs, reward, done = env.time_step(action)\n",
    "  episode_reward += reward\n",
    "  df.append([t, episode_reward, action, obs])\n",
    "  if done:\n",
    "    break\n",
    "\n",
    "print(\"Total reward for simulation under this policy is:\")\n",
    "print (episode_reward)\n",
    "plot_sim(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc944c6a-14e7-42c4-9b19-b8e3acce361e",
   "metadata": {},
   "source": [
    "A natural next step here here would be to build a model of this system to predict future values and guide decisino making.  A simple autoregressive model with Gaussian noise happens to fit the above dynamics very well with few parameters, but provides little insight into how these dynamics would behave under a harvest.  In a longer tutorial, we could try a variety of model estimation and comparison approaches.  \n",
    "\n",
    "\n",
    "Here we take a different approach.  Even in the absence of a specific model, a manager can base harvest effort on past experience.  We define an \"agent\" which observes the estimated stock size each timestep and proposes a harvest effort.  A very simple agent will simply take the same action regardless of the observation. Having experienced several different harvests in the exploration above, you may already have a good idea of what values to try.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48446b-1d7e-4fbb-be56-90ff6a15c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# A simple agent\n",
    "class some_agent:\n",
    "    def __init__(self, effort):\n",
    "        self.effort = effort\n",
    "\n",
    "    def predict(self, obs, **kwargs):\n",
    "        return self.effort\n",
    "\n",
    "# Bob always gives the same action, regardless of the observation:\n",
    "bob = some_agent(.02)\n",
    "bob.predict(.5), bob.predict(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5dcc4a-fef1-4bb1-bd3c-de49e323ba09",
   "metadata": {},
   "source": [
    "Let's see how well \"bob\" does in managing this ecosystem.  For convenience, we'll use a helper function for the simulation so that we don't have to keep writing out the same \"for\" loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240ba8e-0ebb-41e9-ade4-22fa7f7d3db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simulate\n",
    "df, reward = simulate(bob, env)\n",
    "print(reward)\n",
    "plot_sim(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702e5f8-8e85-4282-9713-d810adf31f00",
   "metadata": {},
   "source": [
    "Of course we could imagine other strategies that do react to the observed state.  For instance, let's consider another agent, \"tim\", who uses a very different approach. Tim looks at the observed stock and compares it to some threshold.  If it is above the threshold, tim sends out every fishing boat available -- maximum effort. But if the stock falls below the threshold, tim stops fishing until the stock recovers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaee52c-5f5d-4b33-888b-4b75d6e247e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class another_agent:\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def predict(self, obs, **kwargs):\n",
    "        if obs < self.threshold:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1 \n",
    "\n",
    "\n",
    "# This Agent is all or nothing:\n",
    "tim = another_agent(0.2)\n",
    "\n",
    "tim.predict(.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735d639-4bb6-4638-b547-6cc3ffb2899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df, reward = simulate(tim, env)\n",
    "print(reward)\n",
    "plot_sim(df, geom=\"point\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2dce0-72c7-4c22-bab9-d640185774de",
   "metadata": {},
   "source": [
    "Try modifying the parameters used by the above strategies, or constructing your own.  What is the largest reward you can achieve?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6313cb59-559d-40f0-b567-96c7fcde04a2",
   "metadata": {},
   "source": [
    "\n",
    "While always fitting models to data has become de rigueur, theory can often teach us more by studying whole classes of models analytically.  For instance, for a large class of process-based models which we might postulate and seek to estimate parameters for, the optimal effort is determined by maximum sustainable yield theory, as initially demonstrated simultaneously by economist [Gordon (1954)](https://doi.org/10.1086/257497) & fisheries biologist [Schaefer (1954)](http://hdl.handle.net/1834/21257) in indpendent work in the same year on what would become their epynomous model.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
